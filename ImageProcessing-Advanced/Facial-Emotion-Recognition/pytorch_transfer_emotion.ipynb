{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 28821, 'validation': 7066}\n",
      "device :  cuda:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = \"./images/\"\n",
    "input_shape = 48\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "#data transformation\n",
    "data_transforms = {\n",
    "   'train': transforms.Compose([\n",
    "       transforms.CenterCrop(input_shape),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)\n",
    "   ]),\n",
    "   'validation': transforms.Compose([\n",
    "       transforms.CenterCrop(input_shape),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)\n",
    "   ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "   x: datasets.ImageFolder(\n",
    "       os.path.join(data_dir, x),\n",
    "       transform=data_transforms[x]\n",
    "   )\n",
    "   for x in ['train', 'validation']\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "   x: torch.utils.data.DataLoader(\n",
    "       image_datasets[x], batch_size=64,\n",
    "       shuffle=True, num_workers=4\n",
    "   )\n",
    "   for x in ['train', 'validation']\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
    "\n",
    "print(dataset_sizes)\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device : \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451 28821\n",
      "111 7066\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloaders['train']), len(image_datasets['train']))\n",
    "print(len(dataloaders['validation']), len(image_datasets['validation']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 28821, 'validation': 7066}\n",
      "torch.Size([3, 48, 48])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1df3978ecd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
    "print(dataset_sizes)\n",
    "first_image = image_datasets['train'][0][0]\n",
    "print(first_image.shape)\n",
    "plt.imshow(first_image[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloaders['train']))\n",
    "print(\"image shape : \",images.shape)\n",
    "\n",
    "rows = 4\n",
    "columns = 8\n",
    "fig=plt.figure()\n",
    "for i in range(32):\n",
    "   fig.add_subplot(rows, columns, i+1)\n",
    "   plt.title(class_names[labels[i]])\n",
    "   img = images[i].numpy().transpose((1, 2, 0))\n",
    "   img = std * img + mean\n",
    "   plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception = models.inception_v3(pretrained=True)\n",
    "inception.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=7, bias=True)\n"
     ]
    }
   ],
   "source": [
    "inception_last_layer_input_size = inception.fc.in_features \n",
    "layer = torch.nn.Linear(inception_last_layer_input_size,len(class_names))\n",
    "inception.fc = layer\n",
    "print(inception.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    (7): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Load the model based on resnet18\n",
    "resnet18 = torchvision.models.v(pretrained=True)\n",
    "\n",
    "for param in resnet18.parameters():\n",
    "     param.requires_grad = False\n",
    "     \n",
    "last_layer_input_counter = resnet18.classifier[6].in_features\n",
    "classifier_layers = list(resnet18.classifier.children())[:-1]\n",
    "\n",
    "additional_1 = torch.nn.Linear(last_layer_input_counter, 256)\n",
    "additional_2 = torch.nn.Linear(256, 64)\n",
    "additional_3 = torch.nn.Linear(64,len(class_names))\n",
    "\n",
    "classifier_layers.extend([additional_1,additional_2,additional_3])\n",
    "resnet18.classifier = torch.nn.Sequential(*classifier_layers)\n",
    "\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util():\n",
    "    def training_step(self, batch, model):\n",
    "        images, labels = batch \n",
    "        out = model(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, model):\n",
    "        images, labels = batch \n",
    "        out = model(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = self.accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "    \n",
    "    def accuracy(self,outputs, labels):\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(util, val_loader,model):\n",
    "    model.eval()\n",
    "    outputs = [util.validation_step(batch,model) for batch in val_loader]\n",
    "    return util.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        index = 0\n",
    "        # Training Phase \n",
    "        #model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = util.training_step(batch, model)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            index +=1\n",
    "            if index % 40 == 0:\n",
    "                print(index,\" : \",loss)\n",
    "                \n",
    "        # Validation phase\n",
    "        result = evaluate(util, val_loader,model)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        util.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Util()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30  :  tensor(1.8339, grad_fn=<NllLossBackward>)\n",
      "60  :  tensor(1.8467, grad_fn=<NllLossBackward>)\n",
      "90  :  tensor(1.8649, grad_fn=<NllLossBackward>)\n",
      "120  :  tensor(1.7027, grad_fn=<NllLossBackward>)\n",
      "150  :  tensor(1.8230, grad_fn=<NllLossBackward>)\n",
      "180  :  tensor(1.7966, grad_fn=<NllLossBackward>)\n",
      "210  :  tensor(1.7706, grad_fn=<NllLossBackward>)\n",
      "240  :  tensor(1.7630, grad_fn=<NllLossBackward>)\n",
      "270  :  tensor(1.8487, grad_fn=<NllLossBackward>)\n",
      "300  :  tensor(1.7541, grad_fn=<NllLossBackward>)\n",
      "330  :  tensor(1.6447, grad_fn=<NllLossBackward>)\n",
      "360  :  tensor(1.7877, grad_fn=<NllLossBackward>)\n",
      "390  :  tensor(1.6948, grad_fn=<NllLossBackward>)\n",
      "420  :  tensor(1.7229, grad_fn=<NllLossBackward>)\n",
      "450  :  tensor(1.7401, grad_fn=<NllLossBackward>)\n",
      "Epoch [0], train_loss: 1.7824, val_loss: 1.7052, val_acc: 0.3367\n",
      "30  :  tensor(1.6738, grad_fn=<NllLossBackward>)\n",
      "60  :  tensor(1.6615, grad_fn=<NllLossBackward>)\n",
      "90  :  tensor(1.7084, grad_fn=<NllLossBackward>)\n",
      "120  :  tensor(1.6328, grad_fn=<NllLossBackward>)\n",
      "150  :  tensor(1.7950, grad_fn=<NllLossBackward>)\n",
      "180  :  tensor(1.7033, grad_fn=<NllLossBackward>)\n",
      "210  :  tensor(1.7588, grad_fn=<NllLossBackward>)\n",
      "240  :  tensor(1.7396, grad_fn=<NllLossBackward>)\n",
      "270  :  tensor(1.6824, grad_fn=<NllLossBackward>)\n",
      "300  :  tensor(1.6429, grad_fn=<NllLossBackward>)\n",
      "330  :  tensor(1.6807, grad_fn=<NllLossBackward>)\n",
      "360  :  tensor(1.6066, grad_fn=<NllLossBackward>)\n",
      "390  :  tensor(1.5843, grad_fn=<NllLossBackward>)\n",
      "420  :  tensor(1.7021, grad_fn=<NllLossBackward>)\n",
      "450  :  tensor(1.6148, grad_fn=<NllLossBackward>)\n",
      "Epoch [1], train_loss: 1.6883, val_loss: 1.6651, val_acc: 0.3457\n",
      "30  :  tensor(1.6401, grad_fn=<NllLossBackward>)\n",
      "60  :  tensor(1.7452, grad_fn=<NllLossBackward>)\n",
      "90  :  tensor(1.6190, grad_fn=<NllLossBackward>)\n",
      "120  :  tensor(1.7904, grad_fn=<NllLossBackward>)\n",
      "150  :  tensor(1.5950, grad_fn=<NllLossBackward>)\n",
      "180  :  tensor(1.6244, grad_fn=<NllLossBackward>)\n",
      "210  :  tensor(1.7117, grad_fn=<NllLossBackward>)\n",
      "240  :  tensor(1.6154, grad_fn=<NllLossBackward>)\n",
      "270  :  tensor(1.7309, grad_fn=<NllLossBackward>)\n",
      "300  :  tensor(1.6744, grad_fn=<NllLossBackward>)\n",
      "330  :  tensor(1.6661, grad_fn=<NllLossBackward>)\n",
      "360  :  tensor(1.7093, grad_fn=<NllLossBackward>)\n",
      "390  :  tensor(1.6873, grad_fn=<NllLossBackward>)\n",
      "420  :  tensor(1.7066, grad_fn=<NllLossBackward>)\n",
      "450  :  tensor(1.6781, grad_fn=<NllLossBackward>)\n",
      "Epoch [2], train_loss: 1.6573, val_loss: 1.6428, val_acc: 0.3532\n",
      "30  :  tensor(1.7053, grad_fn=<NllLossBackward>)\n",
      "60  :  tensor(1.6444, grad_fn=<NllLossBackward>)\n",
      "90  :  tensor(1.7051, grad_fn=<NllLossBackward>)\n",
      "120  :  tensor(1.6999, grad_fn=<NllLossBackward>)\n",
      "150  :  tensor(1.7892, grad_fn=<NllLossBackward>)\n",
      "180  :  tensor(1.6834, grad_fn=<NllLossBackward>)\n",
      "210  :  tensor(1.6619, grad_fn=<NllLossBackward>)\n",
      "240  :  tensor(1.6486, grad_fn=<NllLossBackward>)\n",
      "270  :  tensor(1.7031, grad_fn=<NllLossBackward>)\n",
      "300  :  tensor(1.4367, grad_fn=<NllLossBackward>)\n",
      "330  :  tensor(1.5607, grad_fn=<NllLossBackward>)\n",
      "360  :  tensor(1.5512, grad_fn=<NllLossBackward>)\n",
      "390  :  tensor(1.6312, grad_fn=<NllLossBackward>)\n",
      "420  :  tensor(1.5469, grad_fn=<NllLossBackward>)\n",
      "450  :  tensor(1.5726, grad_fn=<NllLossBackward>)\n",
      "Epoch [3], train_loss: 1.6395, val_loss: 1.6280, val_acc: 0.3629\n",
      "30  :  tensor(1.6912, grad_fn=<NllLossBackward>)\n",
      "60  :  tensor(1.7084, grad_fn=<NllLossBackward>)\n",
      "90  :  tensor(1.6707, grad_fn=<NllLossBackward>)\n",
      "120  :  tensor(1.6896, grad_fn=<NllLossBackward>)\n",
      "150  :  tensor(1.6638, grad_fn=<NllLossBackward>)\n",
      "180  :  tensor(1.5188, grad_fn=<NllLossBackward>)\n",
      "210  :  tensor(1.5649, grad_fn=<NllLossBackward>)\n",
      "240  :  tensor(1.6800, grad_fn=<NllLossBackward>)\n",
      "270  :  tensor(1.6368, grad_fn=<NllLossBackward>)\n",
      "300  :  tensor(1.6874, grad_fn=<NllLossBackward>)\n",
      "330  :  tensor(1.5821, grad_fn=<NllLossBackward>)\n",
      "360  :  tensor(1.5568, grad_fn=<NllLossBackward>)\n",
      "390  :  tensor(1.5309, grad_fn=<NllLossBackward>)\n",
      "420  :  tensor(1.6100, grad_fn=<NllLossBackward>)\n",
      "450  :  tensor(1.6506, grad_fn=<NllLossBackward>)\n",
      "Epoch [4], train_loss: 1.6260, val_loss: 1.6196, val_acc: 0.3654\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "opt_func = optim.SGD\n",
    "lr = 0.001\n",
    "history = fit(num_epochs,lr,resnet18,dataloaders['train'],dataloaders['validation'],opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Cuda This Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(dataloaders['train'], device)\n",
    "val_dl = DeviceDataLoader(dataloaders['validation'], device)\n",
    "to_device(inception, device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "opt_func = optim.Adam\n",
    "lr = 0.0001\n",
    "history = fit(num_epochs,lr,inception,train_dl,val_dl,opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet18,\"./model/model_3654\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=256, bias=True)\n",
       "    (7): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (8): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"./model/model_3654\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 1.618986964225769, 'val_acc': 0.3666081726551056}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate(util, dataloaders['validation'],model)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
